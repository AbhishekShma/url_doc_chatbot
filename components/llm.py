from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_core.runnables import RunnableLambda
from typing import List

# Define the model ID (adjust as necessary)
model_id = "meta-llama/Llama-3.2-3B-Instruct"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Ensure that tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

# Load the model
model = AutoModelForCausalLM.from_pretrained(model_id)
model.resize_token_embeddings(len(tokenizer))  # Only needed if fine-tuning

# Create the pipeline
text_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=-1,
    pad_token_id=tokenizer.pad_token_id,
    max_length=1024,
    temperature=0.7,
    truncation=True
)

# Wrap the pipeline in HuggingFacePipeline from Langchain
llm = HuggingFacePipeline(pipeline=text_pipeline)
model = ChatHuggingFace(llm=llm)

# Function to run the model and generate an answer based on the query and documents
def generate_answer(query: str, documents: List[str]) -> str:
    """
    Generate an answer from the model based on the user query and relevant documents.
    
    Args:
        query (str): The user query.
        documents (List[str]): The relevant documents.

    Returns:
        str: The response generated by the model.
    """
    # Construct the prompt with the query and documents
    prompt = f"Answer the following question based on the provided documents:\n\nQuestion: {query}\n\nDocuments:\n"
    for doc in documents:
        prompt += f"- {doc}\n"
    
    # Pass the prompt to the model
    response = model.invoke(prompt)
    return response

# Wrap the function in a RunnableLambda to make it compatible with Langchain's chain
llm_component = RunnableLambda(generate_answer)
